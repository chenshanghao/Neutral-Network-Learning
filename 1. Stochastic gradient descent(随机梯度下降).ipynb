{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**输出:**\n",
    "$$ OUT=(\\sum_{i}x_{i}\\cdot w_{i})+b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**均方差(mean squared error, MSE)损失**:$$ LOSS=(OUT-期望输出)^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最原始的方法是**随机微调参数**。更好的训练方式是**梯度下降(gradient descent)**。可通过**求偏导数**算出往哪个方向调整参数是**“最能使LOSS缩小的方向”**，然后将参数朝这个方向移动一小步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/sgd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于神经网络的任何参数w，使用**梯度下降的训练公式**都是：\n",
    "$$ w^{new}=w-\\eta \\cdot \\frac{\\partial Loss}{\\partial w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据梯度下降公式可得:\n",
    "$$ LOSS=(OUT-期望输出)^{2} $$\n",
    "\n",
    "$$ \\frac{\\partial LOSS}{\\partial OUT} = 2\\cdot (OUT-期望输出) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，对于**权重(weight)**，公式为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{i}^{new} = w_{i} - \\eta \\cdot \\frac{\\partial LOSS}{\\partial w_{i}}=w_{i} - \\eta \\cdot \\frac{\\partial LOSS}{\\partial OUT} \\cdot  \\frac{\\partial OUT}{\\partial w_{i}}=w_{i}-\\eta\\cdot 2(OUT-期望输出)\\cdot x_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，对于**偏置(bias)**,公式为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b^{new} = b - \\eta \\cdot \\frac{\\partial LOSS}{\\partial b}=b - \\eta \\cdot \\frac{\\partial LOSS}{\\partial OUT} \\cdot  \\frac{\\partial OUT}{\\partial b}=b-\\eta\\cdot 2(OUT-期望输出)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据**导数的定义**，对于**任何可导函数f**，有:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(t-\\varepsilon  )\\approx f(t) - \\varepsilon \\cdot \\frac{\\partial f}{\\partial t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，可得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ OUT(w^{new}, x) = OUT(w-\\eta\\cdot \\frac{\\partial LOSS}{\\partial w} , x)\\approx OUT(w, x) - \\eta \\cdot \\frac{\\partial LOSS}{\\partial w}\\cdot \\frac{\\partial OUT}{\\partial w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，可得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ LOSS^{new}=LOSS(OUT(w^{new},x))\\approx LOSS(OUT(w,x)-\\eta \\cdot \\frac{\\partial LOSS}{\\partial w}\\cdot \\frac{\\partial OUT}{\\partial w})\\approx LOSS(OUT(w,x))-\\eta \\cdot \\frac{\\partial LOSS}{\\partial w}\\cdot \\frac{\\partial OUT}{\\partial w}\\cdot \\frac{\\partial LOSS}{\\partial OUT}=LOSS(OUT(w,x))-\\eta \\cdot (\\frac{\\partial LOSS}{\\partial w})^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 公式更新："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{i}^{new} = w_{i}-\\eta\\cdot 2(OUT-期望输出)\\cdot x_{i} $$\n",
    "\n",
    "$$ b^{new} = b-\\eta\\cdot 2(OUT-期望输出)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 重要知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Batch: 每次**调整参数前**所取得的**样本数量**。\n",
    "\n",
    "2) epoch: **每学一遍数据集**，就成为一个epoch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 批量梯度下降法（Batch Gradient Descent，简称BGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体思路是在**更新每一参数时**都使用**所有的样本**来进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**优点：全局最优解；易于并行实现；**\n",
    "\n",
    "**缺点：当样本数目很多时，训练过程会很慢。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 随机梯度下降法(Stochastic gradient descent, 简称SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的具体思路是在**更新每一参数**时都随机使用一个样本来进行更新。SGD伴随的一个问题是**噪音较BGD要多**，使得SGD并不是每次迭代**都向着整体最优化方向**。从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。\n",
    "\n",
    "**优点：训练速度快；**\n",
    "\n",
    "**缺点：准确度下降，并不是全局最优；不易于并行实现。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**更新每一参数时**都使用**一部分样本来进行更新**, 克服上面**两种方法的缺点**，又同时**兼顾两种方法的优点**。\n",
    "\n",
    "如果**样本量比较小**，采用**批量梯度下降算法**。\n",
    "\n",
    "如果**样本太大，或者在线算法**，使用**随机梯度下降算法**。\n",
    "\n",
    "在实际的一般情况下，采用**小批量梯度下降算法**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
